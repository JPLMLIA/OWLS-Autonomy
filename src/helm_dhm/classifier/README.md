# Classifier.py

## Description
This is a basic classifier implementation for rapid research prototyping.

`classifier.py` contains the main functions called by the pipeline.

`compat.py` contains functions from the old classifier that has been kept to
maintain compatibility with the `asdp` step. It should be removed when possible.

## Pending Issues/Notes

- [ ] Gary's kernel projection features have not been implemented
- [ ] The classifier currently ignores any track labels other than `motile`
      and `non-motile`; most notably, tracks labeled `ambiguous` are dropped.
- [ ] The current classifier only uses a random forest model, need to discuss
      how other models and preprocessing should be implemented
- [ ] DOMINE integration

## Usage

### Pipeline

The pipeline only calls `train()` and `predict()`. They are called by the
`train` and `predict` steps, respectively, and rely on outputs from the 
following steps:

- Train:
  - Hand labeled tracks
  - `tracker` for detected tracks
  - `track_evaluation` for matching detected tracks to hand labeled tracks
  - `features` with `--train_feats` flag for extracting features only from
    tracks with matching hand labels
- Predict:
  - `tracker` for detected tracks
  - `features` for extracting features from all tracks
  - Hand labeled tracks of validation metrics are needed

The classifier has outputs in both the batch-level and experiment-level output
directories.

- Train:
  - Trained model: batch-level
  - Training metrics: batch-level
  - Cross validation metrics: batch-level
- Predict:
  - Classification output: experiment-level
  - Prediction metrics (if they exist): batch-level

### `train()`

```
def train(experiments, batch_outdir, config, hyperparams={"max_depth": 5}):
    """ Trains an sklearn random forest model on input features and saves it as a pickle

    Parameters
    ----------
    experiments: list
        List of experiments generated by pipeline-level glob
    batch_outdir: string
        Output directory for batch-level metrics and trained model
    config: dict
        Configuration dictionary read in by pipeline from YAML
    hyperparams: dict
        Hyperparameters for model training. Exposed for DOMINE optimization.
        NOTE: Temporarily defaults to {"max_depth": 5}
        NOTE: Do not add hyperparameters to config, as it will be fixed eventually
    
    Returns
    -------
    None
    TODO: Return metrics for DOMINE optimization
    """
```

Notes:
* `hyperparams` was exposed to allow for hyperparameter tuning for DOMINE.
  However, additional parameters and returns are needed for full integration:
  * For certain models, DOMINE may want to be able to optimize which features
    are included. An optional parameter to specify feature masing outside of
    the config file should be useful.
  * DOMINE may want metrics from cross-validation to optimize - these should be
    included in the metrics return in the end so DOMINE doesn't need to read the
    output metric file.
* Even if cross-validation is enabled, the model that is saved by this step is
  the model trained on the entire training set. In the future, we may want to
  save the best fold from cross-validation.
* Features with `NaN` or `inf` values are converted using `np.nan_to_num()`
  before classification.
* Training will abort if:
  * There are no tracks in the `features` output
  * None of the tracks in the `features` output have valid labels
* Cross-validation will abort if there are not enough samples to do the number
  of folds specified. However, training itself will not abort.

### `predict()`

```
def predict(experiments, batch_outdir, model_path, config):
    """ Tests an sklearn model on input features and writes prediction JSONs

    Parameters
    ----------
    experiments: list
        List of experiments generated by pipeline-level glob
    batch_outdir: string
        Batch outdir specified in --batch_outdir
    model_path: string
        Absolute path to the model to be used for prediction
    config: dict
        Configuration dictionary read in by pipeline from YAML
    
    Returns
    -------
    None
    TODO: Return metrics for DOMINE optimization?
    """
```

Notes
* This function is theoretically model-agnostic for any sklearn model - this
  should be tested as classifier research continues.
* The classification output only includes the modified track JSON files with the
  additional keys `classification` and `probability_motility`. The CSV file from
  the old classifier was not included as nothing was using it.
* Metrics are written only for tracks in the input that have hand labels.
* This function may also want to return metrics for DOMINE.