# Labelbox Dataset Pipeline

This directory contains scripts to prepare and upload datasets to Labelbox.

## General Tools

### label_swap_util.py

Converts pre-labelbox labels into the "labelbox format" generated by 
`labelbox_parse.py`. The HELM/FAME pipeline expects this format for evaluation.

```
usage: label_swap_util.py [-h] --experiments_glob EXPERIMENTS_GLOB
                          --label_subdir LABEL_SUBDIR

optional arguments:
  -h, --help            show this help message and exit
  --experiments_glob EXPERIMENTS_GLOB
                        Glob of experiments which need labels swapped
  --label_subdir LABEL_SUBDIR
                        Subdir within each experiment for which labels exist
```

## Labelbox Upload Pipeline w/ Signed S3 URLs

Note that, if Labelbox is given delegated access to S3, a different procedure
will be needed.

### 1. Standardize Experiment Names

For consistency and sanity, convert the experiment names in your dataset into
a standardied external ID with `labelbox_standardize.py`

```
usage: labelbox_standardize.py [-h] [--force] dataset_dir

positional arguments:
  dataset_dir  Directory of experiments to standardize

optional arguments:
  -h, --help   show this help message and exit
  --force      Don't require use confirmation for each rename
```

The script expects a top-level directory with the file structure:
```
dataset_dir
├── YYYY_MM_DD_HH_MM_SS
│   ├── YYYY_MM_DD_HH_MM_SS_mhi_labeled.png
│   └── YYYY_MM_DD_HH_MM_SS_base_movie.mp4
├── YYYY.MM.DD_HH.MM.SS_addtl_labels
│   ├── YYYY.MM.DD_HH.MM.SS_addtl_labels_mhi_labeled.png
│   └── YYYY.MM.DD_HH.MM.SS_addtl_labels_base_movie.mp4
```

It then splits on all underscores and periods to standardize all experiment
names to the format `YYYY_MM_DD_HH_MM_SS`. For example,
```
round_1
├── 2019_06_21_17_13_33
│   ├── 2019_06_21_17_13_33_mhi.png
│   └── 2019_06_21_17_13_33_movie.mp4
├── 2019_06_24_19_25_27
│   ├── 2019_06_24_19_25_27_mhi.png
│   └── 2019_06_24_19_25_27_movie.mp4
```

Because experiment naming formats can be varied and unreliable, the script
asks for user confirmation for each and every rename. Declining gives the
user the opportunity to rename manually or adjust the script.

### 2. Upload Dataset to S3

After dataset standardization, the dataset can be uploaded to S3 using
`S3_upload.py`

**Dependencies**
- AWS CLI
- boto3

```
usage: S3_upload.py [-h] [--bucket BUCKET] [--aws_profile AWS_PROFILE]
                    dataset_dir

positional arguments:
  dataset_dir           Directory of experiments to upload to S3

optional arguments:
  -h, --help            show this help message and exit
  --bucket BUCKET       Name of AWS S3 bucket to upload data to
  --aws_profile AWS_PROFILE
                        Name of AWS profile to use for credentials
```

**S3-side Setup**
1.  Your project must have a pubcloud AWS account. Contact your sysadmin or the
    JPL AWS team (referred to as the SAs from here on) for setup.
2.  There must be an existing S3 bucket instantiated. Create a bucket or ask
    the SAs to create one.
3.  (Required for signed URLs) Ask the SAs to set the
    Labelbox-required CORS headers on the bucket.
    - Reference: https://docs.labelbox.com/en/how-to-guides/hybrid-cloud#create-cors-headers
4.  (Required for signed URLs) Ask the SAs to create a service account with
    read-only access that can sign URLs for up to 7 days.

**Local Setup**
1.  Install the AWS CLI
    - Reference: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html
2.  Set up a profile with your JPL SSO account. This will expire in 4 hours.
    - Tool installation reference: https://github.jpl.nasa.gov/cloud/Access-Key-Generation/blob/master/README.md
    - Run `aws-login.darwin.amd64 --region us-west-1` (adjust for your cloud
    region)

You can now run the script. Each object (movie or MHI) is uploaded with the key
`dataset_dir/filename`
This is the entire object key string - AWS S3 does not have hierarchical 
structure. It does, however, support folders as prefixes. 
Reference: https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html

### 3. Upload Dataset to Labelbox

After videos and MHIs exist on S3 as objects, the dataset can be uploaded to
Labelbox using `S3_to_labelbox.py`

**Dependencies**
- AWS CLI
- boto3
- labelbox SDK

```
usage: S3_to_labelbox.py [-h] config

positional arguments:
  config      Configuration for S3 and Labelbox

optional arguments:
  -h, --help  show this help message and exit
```

This script requires a YAML configuration file with the following:

```
# S3 configs
s3_bucket: <name of the S3 bucket>
s3_profile: <name of the service account with 7d sign expiration>
s3_prefix: <prefix/folder of the dataset you want to upload>

# Labelbox configs
lb_dataset: <name of the dataset on Labelbox>
LB_API_KEY: <Labelbox API key>
LB_ENDPOINT: https://api.labelbox.com/graphql
```

The Labelbox dataset name must not already exist, otherwise it will attempt to
update it. The Labelbox API key can be generated following: https://docs.labelbox.com/en/how-to-guides/create-an-api-key

Upon running, the script will state that the specified dataset name does not
exist on Labelbox, and ask if you want to create one. Confirm with `y` and it
will create the dataset with S3 URLs.

The dataset still needs to be linked to a project afterwards. Do this through
the Labelbox website.

### 4. Update Dataset URLs on Labelbox

Datasets created with signed URLs expire after 7 days if signed with the
service account, and 4 hours if signed with a personal JPL SSO account.
Therefore, the URLs must be updated before expiry. This can be done without
losing object permanence and associated labels.

Run `S3_to_labelbox.py` again. This time, it will state that it found an
existing dataset and print its UID. It will then ask you to confirm that you
want to update the signed URLs in this dataset. That's all!

## Labelbox Upload Pipeline w/ Delegated Access

JPL AWS team seems to be going fairly quickly on this, so plan for this
implementation in the next couple weeks.

## LabelBox Download Cheat Sheet:
```
1.) Ensure you have a LabelBox API key.
        Instructions to get one are here:
        https://labelbox.com/docs/api/getting-started
2.) Download the label metadata from Labelbox.  These are found
        under project->export.  Keep json selected for format. This
        becomes the --label_metadata_file argument.
3.) Results will be placed in --experiment_dir folder. Future versions of
        this script can place them in the experiment subdir directly, but
        for now they are placed at the root of the experiment_dir folder 
```